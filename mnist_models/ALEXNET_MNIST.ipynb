{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSQklKFCjQcZ"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation, Dropout\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AlexNet Model\n",
        "def build_alexnet(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        # Layer 1\n",
        "        Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'), # Adjusted stride and added padding\n",
        "\n",
        "        # Layer 2\n",
        "        Conv2D(256, (5, 5), padding='same', activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'), # Adjusted stride and added padding\n",
        "\n",
        "        # Layer 3\n",
        "        Conv2D(384, (3, 3), padding='same', activation='relu'),\n",
        "\n",
        "        # Layer 4\n",
        "        Conv2D(384, (3, 3), padding='same', activation='relu'),\n",
        "\n",
        "        # Layer 5\n",
        "        Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'), # Adjusted stride and added padding\n",
        "\n",
        "        Flatten(),\n",
        "\n",
        "        # Layer 6\n",
        "        Dense(4096, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        # Layer 7\n",
        "        Dense(4096, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        # Output layer\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "hAuJM33q0Bdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "input_shape = (28, 28, 1)\n",
        "num_classes = 10\n",
        "train_images = train_images.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "test_images = test_images.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "train_labels = to_categorical(train_labels, num_classes)\n",
        "test_labels = to_categorical(test_labels, num_classes)\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.1, random_state=42)\n",
        "\n",
        "# Build the AlexNet-like model for MNIST\n",
        "model = build_alexnet(input_shape, num_classes)\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define the ModelCheckpoint callback to save the best model\n",
        "checkpoint_filepath = 'alexnet.h5'\n",
        "model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                            save_best_only=True,\n",
        "                                            monitor='val_accuracy',\n",
        "                                            mode='max',\n",
        "                                            verbose=1)\n",
        "\n",
        "# Fit the model to the training data with validation data and ModelCheckpoint callback\n",
        "history = model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_data=(val_images, val_labels),\n",
        "                    callbacks=[model_checkpoint_callback])\n",
        "\n",
        "# Define a new model without the output layer\n",
        "output_layer_index = -2  # Index of the layer before the output layer\n",
        "feature_extractor = Model(inputs=model.inputs, outputs=model.layers[output_layer_index].output)\n",
        "\n",
        "# Save the model architecture and weights\n",
        "feature_extractor.save('alexnet_feature_extractor.h5')\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "best_model = tf.keras.models.load_model(checkpoint_filepath)\n",
        "test_loss, test_accuracy = best_model.evaluate(test_images, test_labels, verbose=2)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEeK3fxtz3p1",
        "outputId": "52d1645c-1d8a-4776-d0b9-4d61e7ba0070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_5 (Conv2D)           (None, 5, 5, 96)          11712     \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 5, 5, 96)          384       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 3, 3, 96)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 3, 3, 256)         614656    \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 3, 3, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPoolin  (None, 2, 2, 256)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 2, 2, 384)         885120    \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 2, 2, 384)         1327488   \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 2, 2, 256)         884992    \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 2, 2, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPoolin  (None, 1, 1, 256)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 4096)              1052672   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 4096)              16781312  \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21601354 (82.40 MB)\n",
            "Trainable params: 21600138 (82.40 MB)\n",
            "Non-trainable params: 1216 (4.75 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.3970 - accuracy: 0.8866\n",
            "Epoch 1: val_accuracy improved from -inf to 0.90583, saving model to alexnet.h5\n",
            "422/422 [==============================] - 12s 22ms/step - loss: 0.3962 - accuracy: 0.8868 - val_loss: 0.3240 - val_accuracy: 0.9058\n",
            "Epoch 2/10\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.1079 - accuracy: 0.9719\n",
            "Epoch 2: val_accuracy improved from 0.90583 to 0.96517, saving model to alexnet.h5\n",
            "422/422 [==============================] - 9s 21ms/step - loss: 0.1080 - accuracy: 0.9719 - val_loss: 0.1289 - val_accuracy: 0.9652\n",
            "Epoch 3/10\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0878 - accuracy: 0.9769\n",
            "Epoch 3: val_accuracy did not improve from 0.96517\n",
            "422/422 [==============================] - 8s 19ms/step - loss: 0.0884 - accuracy: 0.9769 - val_loss: 0.1251 - val_accuracy: 0.9643\n",
            "Epoch 4/10\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0692 - accuracy: 0.9816\n",
            "Epoch 4: val_accuracy did not improve from 0.96517\n",
            "422/422 [==============================] - 8s 19ms/step - loss: 0.0692 - accuracy: 0.9816 - val_loss: 0.1412 - val_accuracy: 0.9642\n",
            "Epoch 5/10\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0594 - accuracy: 0.9844\n",
            "Epoch 5: val_accuracy improved from 0.96517 to 0.98533, saving model to alexnet.h5\n",
            "422/422 [==============================] - 9s 21ms/step - loss: 0.0595 - accuracy: 0.9844 - val_loss: 0.0674 - val_accuracy: 0.9853\n",
            "Epoch 6/10\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0498 - accuracy: 0.9870\n",
            "Epoch 6: val_accuracy did not improve from 0.98533\n",
            "422/422 [==============================] - 8s 19ms/step - loss: 0.0505 - accuracy: 0.9869 - val_loss: 0.1429 - val_accuracy: 0.9630\n",
            "Epoch 7/10\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0474 - accuracy: 0.9880\n",
            "Epoch 7: val_accuracy did not improve from 0.98533\n",
            "422/422 [==============================] - 8s 19ms/step - loss: 0.0475 - accuracy: 0.9879 - val_loss: 0.0726 - val_accuracy: 0.9818\n",
            "Epoch 8/10\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0462 - accuracy: 0.9887\n",
            "Epoch 8: val_accuracy did not improve from 0.98533\n",
            "422/422 [==============================] - 8s 19ms/step - loss: 0.0462 - accuracy: 0.9887 - val_loss: 0.0642 - val_accuracy: 0.9825\n",
            "Epoch 9/10\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0363 - accuracy: 0.9905\n",
            "Epoch 9: val_accuracy did not improve from 0.98533\n",
            "422/422 [==============================] - 8s 19ms/step - loss: 0.0364 - accuracy: 0.9905 - val_loss: 0.0687 - val_accuracy: 0.9837\n",
            "Epoch 10/10\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0521 - accuracy: 0.9892\n",
            "Epoch 10: val_accuracy did not improve from 0.98533\n",
            "422/422 [==============================] - 8s 19ms/step - loss: 0.0520 - accuracy: 0.9892 - val_loss: 0.0670 - val_accuracy: 0.9817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 2s - loss: 0.0608 - accuracy: 0.9850 - 2s/epoch - 6ms/step\n",
            "Test Loss: 0.0608\n",
            "Test Accuracy: 0.9850\n"
          ]
        }
      ]
    }
  ]
}