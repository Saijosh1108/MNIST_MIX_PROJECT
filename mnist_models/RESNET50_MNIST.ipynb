{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isMw6aeMz62g"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation, Dropout\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ResNet50 Model\n",
        "def build_resnet50(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        # Layer 1\n",
        "        Conv2D(64, (7, 7), strides=(2, 2), activation='relu', padding='same', input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'),\n",
        "\n",
        "        # Layer 2\n",
        "        Conv2D(64, (1, 1), activation='relu'),\n",
        "        Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
        "        Conv2D(256, (1, 1), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'),\n",
        "\n",
        "        # Layer 3\n",
        "        Conv2D(128, (1, 1), activation='relu'),\n",
        "        Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "        Conv2D(512, (1, 1), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        # Layer 4\n",
        "        Conv2D(256, (1, 1), activation='relu'),\n",
        "        Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
        "        Conv2D(1024, (1, 1), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        # Layer 5\n",
        "        Conv2D(512, (1, 1), activation='relu'),\n",
        "        Conv2D(512, (3, 3), padding='same', activation='relu'),\n",
        "        Conv2D(2048, (1, 1), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'),\n",
        "\n",
        "        # Flatten\n",
        "        Flatten(),\n",
        "\n",
        "        # Layer 6\n",
        "        Dense(2048, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        # Layer 7\n",
        "        Dense(2048, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        # Output layer\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "yQEog5Is0u41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "input_shape = (28, 28, 1)\n",
        "num_classes = 10\n",
        "train_images = train_images.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "test_images = test_images.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "train_labels = to_categorical(train_labels, num_classes)\n",
        "test_labels = to_categorical(test_labels, num_classes)\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.1, random_state=42)\n",
        "\n",
        "# Build the AlexNet-like model for MNIST\n",
        "model = build_resnet50(input_shape, num_classes)\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define the ModelCheckpoint callback to save the best model\n",
        "checkpoint_filepath = 'resnet50.h5'\n",
        "model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                            save_best_only=True,\n",
        "                                            monitor='val_accuracy',\n",
        "                                            mode='max',\n",
        "                                            verbose=1)\n",
        "\n",
        "# Fit the model to the training data with validation data and ModelCheckpoint callback\n",
        "history = model.fit(train_images, train_labels, epochs=15, batch_size=128, validation_data=(val_images, val_labels),\n",
        "                    callbacks=[model_checkpoint_callback])\n",
        "\n",
        "# Define a new model without the output layer\n",
        "output_layer_index = -2  # Index of the layer before the output layer\n",
        "feature_extractor = Model(inputs=model.inputs, outputs=model.layers[output_layer_index].output)\n",
        "\n",
        "# Save the model architecture and weights\n",
        "feature_extractor.save('resnet50_feature_extractor.h5')\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "best_model = tf.keras.models.load_model(checkpoint_filepath)\n",
        "test_loss, test_accuracy = best_model.evaluate(test_images, test_labels, verbose=2)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvgVsOiG04po",
        "outputId": "9b5bb9d9-8c3e-44df-c099-846e70c9b5b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 14, 14, 64)        3200      \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 14, 14, 64)        256       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 7, 7, 64)          0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 7, 7, 64)          4160      \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 7, 7, 64)          36928     \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 7, 7, 256)         16640     \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 7, 7, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 4, 4, 256)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 4, 4, 128)         32896     \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 4, 4, 128)         147584    \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 4, 4, 512)         66048     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 4, 4, 512)         2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 4, 4, 256)         131328    \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 4, 4, 256)         590080    \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 4, 4, 1024)        263168    \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 4, 4, 1024)        4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 4, 4, 512)         524800    \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 4, 4, 2048)        1050624   \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 4, 4, 2048)        8192      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 2, 2, 2048)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 8192)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2048)              16779264  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2048)              4196352   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 26238986 (100.09 MB)\n",
            "Trainable params: 26231178 (100.06 MB)\n",
            "Non-trainable params: 7808 (30.50 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/15\n",
            "422/422 [==============================] - ETA: 0s - loss: 1.0665 - accuracy: 0.7241\n",
            "Epoch 1: val_accuracy improved from -inf to 0.11417, saving model to resnet50.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r422/422 [==============================] - 33s 44ms/step - loss: 1.0665 - accuracy: 0.7241 - val_loss: 6.5240 - val_accuracy: 0.1142\n",
            "Epoch 2/15\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.1730 - accuracy: 0.9575\n",
            "Epoch 2: val_accuracy improved from 0.11417 to 0.96250, saving model to resnet50.h5\n",
            "422/422 [==============================] - 16s 38ms/step - loss: 0.1730 - accuracy: 0.9575 - val_loss: 0.1744 - val_accuracy: 0.9625\n",
            "Epoch 3/15\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.1096 - accuracy: 0.9743\n",
            "Epoch 3: val_accuracy improved from 0.96250 to 0.97350, saving model to resnet50.h5\n",
            "422/422 [==============================] - 16s 39ms/step - loss: 0.1094 - accuracy: 0.9744 - val_loss: 0.1012 - val_accuracy: 0.9735\n",
            "Epoch 4/15\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0778 - accuracy: 0.9817\n",
            "Epoch 4: val_accuracy did not improve from 0.97350\n",
            "422/422 [==============================] - 16s 37ms/step - loss: 0.0778 - accuracy: 0.9817 - val_loss: 0.1055 - val_accuracy: 0.9713\n",
            "Epoch 5/15\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0672 - accuracy: 0.9838\n",
            "Epoch 5: val_accuracy improved from 0.97350 to 0.98117, saving model to resnet50.h5\n",
            "422/422 [==============================] - 16s 39ms/step - loss: 0.0671 - accuracy: 0.9839 - val_loss: 0.0772 - val_accuracy: 0.9812\n",
            "Epoch 6/15\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.4854 - accuracy: 0.9113\n",
            "Epoch 6: val_accuracy did not improve from 0.98117\n",
            "422/422 [==============================] - 15s 36ms/step - loss: 0.4851 - accuracy: 0.9113 - val_loss: 3.6327 - val_accuracy: 0.4055\n",
            "Epoch 7/15\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.1922 - accuracy: 0.9468\n",
            "Epoch 7: val_accuracy did not improve from 0.98117\n",
            "422/422 [==============================] - 15s 36ms/step - loss: 0.1921 - accuracy: 0.9468 - val_loss: 0.1918 - val_accuracy: 0.9398\n",
            "Epoch 8/15\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.1076 - accuracy: 0.9704\n",
            "Epoch 8: val_accuracy did not improve from 0.98117\n",
            "422/422 [==============================] - 15s 36ms/step - loss: 0.1076 - accuracy: 0.9704 - val_loss: 0.0730 - val_accuracy: 0.9788\n",
            "Epoch 9/15\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0803 - accuracy: 0.9782\n",
            "Epoch 9: val_accuracy did not improve from 0.98117\n",
            "422/422 [==============================] - 15s 36ms/step - loss: 0.0803 - accuracy: 0.9781 - val_loss: 0.1033 - val_accuracy: 0.9725\n",
            "Epoch 10/15\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0709 - accuracy: 0.9809\n",
            "Epoch 10: val_accuracy did not improve from 0.98117\n",
            "422/422 [==============================] - 15s 36ms/step - loss: 0.0708 - accuracy: 0.9809 - val_loss: 0.0865 - val_accuracy: 0.9747\n",
            "Epoch 11/15\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0636 - accuracy: 0.9826\n",
            "Epoch 11: val_accuracy did not improve from 0.98117\n",
            "422/422 [==============================] - 15s 37ms/step - loss: 0.0637 - accuracy: 0.9826 - val_loss: 0.0767 - val_accuracy: 0.9788\n",
            "Epoch 12/15\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0567 - accuracy: 0.9846\n",
            "Epoch 12: val_accuracy did not improve from 0.98117\n",
            "422/422 [==============================] - 15s 36ms/step - loss: 0.0566 - accuracy: 0.9846 - val_loss: 0.0990 - val_accuracy: 0.9730\n",
            "Epoch 13/15\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0498 - accuracy: 0.9865\n",
            "Epoch 13: val_accuracy did not improve from 0.98117\n",
            "422/422 [==============================] - 15s 35ms/step - loss: 0.0499 - accuracy: 0.9865 - val_loss: 0.1025 - val_accuracy: 0.9740\n",
            "Epoch 14/15\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0482 - accuracy: 0.9870\n",
            "Epoch 14: val_accuracy did not improve from 0.98117\n",
            "422/422 [==============================] - 15s 35ms/step - loss: 0.0481 - accuracy: 0.9871 - val_loss: 0.1346 - val_accuracy: 0.9707\n",
            "Epoch 15/15\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0490 - accuracy: 0.9869\n",
            "Epoch 15: val_accuracy did not improve from 0.98117\n",
            "422/422 [==============================] - 15s 36ms/step - loss: 0.0492 - accuracy: 0.9869 - val_loss: 0.0805 - val_accuracy: 0.9788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 3s - loss: 0.0731 - accuracy: 0.9815 - 3s/epoch - 9ms/step\n",
            "Test Loss: 0.0731\n",
            "Test Accuracy: 0.9815\n"
          ]
        }
      ]
    }
  ]
}