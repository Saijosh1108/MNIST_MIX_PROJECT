{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6VEKu5Cpwmw"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import mnist\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG16 Model\n",
        "def build_vgg16(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        # Block 1\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D((2, 2), strides=(2, 2)),\n",
        "\n",
        "        # Block 2\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D((2, 2), strides=(2, 2)),\n",
        "\n",
        "        # Block 3\n",
        "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D((2, 2), strides=(2, 2)),\n",
        "\n",
        "        # Block 4\n",
        "        Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "        Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "        Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D((2, 2), strides=(2, 2)),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(4096, activation='relu'),\n",
        "        Dense(4096, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "uY4nU2J_rYyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "input_shape = (28, 28, 1)\n",
        "num_classes = 10\n",
        "train_images = train_images.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "test_images = test_images.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "train_labels = to_categorical(train_labels, num_classes)\n",
        "test_labels = to_categorical(test_labels, num_classes)\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "# You can use any method to split the data, such as train_test_split from sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.1, random_state=42)\n",
        "\n",
        "# Build the VGG16-like model for MNIST\n",
        "model = build_vgg16(input_shape, num_classes)\n",
        "model.summary();\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define the ModelCheckpoint callback to save the best model\n",
        "checkpoint_filepath = 'best_model.h5'\n",
        "model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                            save_best_only=True,\n",
        "                                            monitor='val_accuracy',\n",
        "                                            mode='max',\n",
        "                                            verbose=1)\n",
        "\n",
        "# Fit the model to the training data with validation data and ModelCheckpoint callback\n",
        "history = model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_data=(val_images, val_labels),\n",
        "                    callbacks=[model_checkpoint_callback])\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "best_model = tf.keras.models.load_model(checkpoint_filepath)\n",
        "test_loss, test_accuracy = best_model.evaluate(test_images, test_labels, verbose=2)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMIr-JbouWYa",
        "outputId": "9ebb1eb3-5651-410e-c2d8-126767ac7026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_40 (Conv2D)          (None, 28, 28, 64)        640       \n",
            "                                                                 \n",
            " conv2d_41 (Conv2D)          (None, 28, 28, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_16 (MaxPooli  (None, 14, 14, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_42 (Conv2D)          (None, 14, 14, 128)       73856     \n",
            "                                                                 \n",
            " conv2d_43 (Conv2D)          (None, 14, 14, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d_17 (MaxPooli  (None, 7, 7, 128)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_44 (Conv2D)          (None, 7, 7, 256)         295168    \n",
            "                                                                 \n",
            " conv2d_45 (Conv2D)          (None, 7, 7, 256)         590080    \n",
            "                                                                 \n",
            " conv2d_46 (Conv2D)          (None, 7, 7, 256)         590080    \n",
            "                                                                 \n",
            " max_pooling2d_18 (MaxPooli  (None, 3, 3, 256)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_47 (Conv2D)          (None, 3, 3, 512)         1180160   \n",
            "                                                                 \n",
            " conv2d_48 (Conv2D)          (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " conv2d_49 (Conv2D)          (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " max_pooling2d_19 (MaxPooli  (None, 1, 1, 512)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 4096)              2101248   \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 4096)              16781312  \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 26557642 (101.31 MB)\n",
            "Trainable params: 26557642 (101.31 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "421/422 [============================>.] - ETA: 0s - loss: 1.0745 - accuracy: 0.5976\n",
            "Epoch 1: val_accuracy improved from -inf to 0.96917, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r422/422 [==============================] - 24s 49ms/step - loss: 1.0726 - accuracy: 0.5983 - val_loss: 0.1072 - val_accuracy: 0.9692\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - ETA: 0s - loss: 0.0886 - accuracy: 0.9756\n",
            "Epoch 2: val_accuracy improved from 0.96917 to 0.98450, saving model to best_model.h5\n",
            "422/422 [==============================] - 21s 49ms/step - loss: 0.0886 - accuracy: 0.9756 - val_loss: 0.0556 - val_accuracy: 0.9845\n",
            "Epoch 3/10\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0605 - accuracy: 0.9833\n",
            "Epoch 3: val_accuracy improved from 0.98450 to 0.98600, saving model to best_model.h5\n",
            "422/422 [==============================] - 20s 48ms/step - loss: 0.0605 - accuracy: 0.9833 - val_loss: 0.0512 - val_accuracy: 0.9860\n",
            "Epoch 4/10\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0472 - accuracy: 0.9870\n",
            "Epoch 4: val_accuracy did not improve from 0.98600\n",
            "422/422 [==============================] - 19s 46ms/step - loss: 0.0473 - accuracy: 0.9870 - val_loss: 0.0640 - val_accuracy: 0.9830\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9884\n",
            "Epoch 5: val_accuracy improved from 0.98600 to 0.98983, saving model to best_model.h5\n",
            "422/422 [==============================] - 22s 52ms/step - loss: 0.0433 - accuracy: 0.9884 - val_loss: 0.0392 - val_accuracy: 0.9898\n",
            "Epoch 6/10\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0381 - accuracy: 0.9902\n",
            "Epoch 6: val_accuracy did not improve from 0.98983\n",
            "422/422 [==============================] - 19s 46ms/step - loss: 0.0381 - accuracy: 0.9902 - val_loss: 0.0573 - val_accuracy: 0.9863\n",
            "Epoch 7/10\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0396 - accuracy: 0.9899\n",
            "Epoch 7: val_accuracy improved from 0.98983 to 0.99083, saving model to best_model.h5\n",
            "422/422 [==============================] - 20s 48ms/step - loss: 0.0395 - accuracy: 0.9899 - val_loss: 0.0426 - val_accuracy: 0.9908\n",
            "Epoch 8/10\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0265 - accuracy: 0.9932\n",
            "Epoch 8: val_accuracy did not improve from 0.99083\n",
            "422/422 [==============================] - 19s 46ms/step - loss: 0.0264 - accuracy: 0.9932 - val_loss: 0.0497 - val_accuracy: 0.9897\n",
            "Epoch 9/10\n",
            "421/422 [============================>.] - ETA: 0s - loss: 0.0342 - accuracy: 0.9918\n",
            "Epoch 9: val_accuracy did not improve from 0.99083\n",
            "422/422 [==============================] - 19s 46ms/step - loss: 0.0343 - accuracy: 0.9918 - val_loss: 0.0412 - val_accuracy: 0.9883\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9939\n",
            "Epoch 10: val_accuracy improved from 0.99083 to 0.99133, saving model to best_model.h5\n",
            "422/422 [==============================] - 25s 59ms/step - loss: 0.0246 - accuracy: 0.9939 - val_loss: 0.0475 - val_accuracy: 0.9913\n",
            "313/313 - 2s - loss: 0.0425 - accuracy: 0.9911 - 2s/epoch - 7ms/step\n",
            "Test Loss: 0.0425\n",
            "Test Accuracy: 0.9911\n"
          ]
        }
      ]
    }
  ]
}